{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Seriation-Classification:--sc-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Seriation Classification:  sc-1</a></div><div class=\"lev2\"><a href=\"#Initial-Classification-Attempt\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Initial Classification Attempt</a></div><div class=\"lev2\"><a href=\"#Leave-One-Out-Cross-Validation-for-Selecting-Optimal-K\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Leave-One-Out Cross Validation for Selecting Optimal K</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seriation Classification:  sc-1 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of experiment `sc-1` is to validate that the Laplacian eigenvalue spectral distance can be useful in k-Nearest Neighbor classifiers for seriation output.  In this experiment, I take a supervised learning approach, starting with two regional metapopulation models, simulating unbiased cultural transmission with 50 replicates across each model, sampling and time averaging the resulting cultural trait distributions in archaeologically realistic ways, and then seriating the results using our IDSS algorithm.  Each seriation resulting from this procedure is thus \"labeled\" as to the regional metapopulation model from which it originated, so we can assess the accuracy of predicting that label based upon the graph spectral similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:22:33.964109",
     "start_time": "2016-02-23T09:22:32.042017"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import cPickle as pickle\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:22:34.122866",
     "start_time": "2016-02-23T09:22:33.970680"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graphs = pickle.load(open(\"train-freq-graphs.pkl\",'r'))\n",
    "train_labels = pickle.load(open(\"train-freq-labels.pkl\",'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn-mmadsen is a python package of useful machine learning tools that I'm accumulating for research and commercial work.  You can find it at http://github.com/mmadsen/sklearn-mmadsen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:22:36.866651",
     "start_time": "2016-02-23T09:22:36.857846"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn_mmadsen.graphs as skm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Classification Attempt ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just see if the graph spectral distance does anything useful at all, or whether I'm barking up the wrong tree.  I imagine that we want a few neighbors (to rule out relying on a single neighbor which might be anomalous), but not too many.  So let's start with k=5.  \n",
    "\n",
    "The approach here is to essentially do a \"leave one out\" strategy on the dataset.  The KNN model isn't really \"trained\" in the usual sense of the term, so we don't need to separate a test and train set, we just need to make sure that the target graph we're trying to predict is not one of the \"training\" graphs that we calculate spectral distances to, otherwise the self-matching of the graph will always predict zero distance.  So we first define a simple function which splits a graph out of the training set and returns the rest.  I'd use scikit-learn functions for this, but our \"data\" is really a list of NetworkX objects, not a numeric matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:22:39.628097",
     "start_time": "2016-02-23T09:22:39.625961"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gclf = skm.GraphEigenvalueNearestNeighbors(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:22:42.344478",
     "start_time": "2016-02-23T09:22:42.338773"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leave_one_out_cv(ix, train_graphs, train_labels):\n",
    "    \"\"\"\n",
    "    Simple LOO data sets for kNN classification, given an index, returns a train set, labels, with the left out \n",
    "    graph and label as test_graph, test_label\n",
    "    \"\"\"\n",
    "    test_graph = train_graphs[ix]\n",
    "    test_label = train_labels[ix]\n",
    "    train_loo_graphs = deepcopy(train_graphs)\n",
    "    train_loo_labels = deepcopy(train_labels)\n",
    "    del train_loo_graphs[ix]\n",
    "    del train_loo_labels[ix]\n",
    "    return (train_loo_graphs, train_loo_labels, test_graph, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:23:20.305174",
     "start_time": "2016-02-23T09:22:44.289728"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for ix in range(0, len(train_graphs)):\n",
    "    train_loo_graphs, train_loo_labels, test_graph, test_label = leave_one_out_cv(ix, train_graphs, train_labels)\n",
    "    gclf.fit(train_loo_graphs, train_loo_labels)\n",
    "    test_pred.append(gclf.predict([test_graph])[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:23:26.948388",
     "start_time": "2016-02-23T09:23:26.921634"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          predicted 0  predicted 1\n",
      "actual 0           41            9\n",
      "actual 1           14           35\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.82      0.78        50\n",
      "          1       0.80      0.71      0.75        49\n",
      "\n",
      "avg / total       0.77      0.77      0.77        99\n",
      "\n",
      "Accuracy on test: 0.768\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(train_labels, test_pred)\n",
    "cmdf = pd.DataFrame(cm)\n",
    "cmdf.columns = map(lambda x: 'predicted {}'.format(x), cmdf.columns)\n",
    "cmdf.index = map(lambda x: 'actual {}'.format(x), cmdf.index)\n",
    "\n",
    "print cmdf\n",
    "print(classification_report(train_labels, test_pred))\n",
    "print(\"Accuracy on test: %0.3f\" % accuracy_score(train_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:23:30.472779",
     "start_time": "2016-02-23T09:23:30.265367"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10642a350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD9CAYAAAB3NXH8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngBmiB8wQPWCG6AEz/wbHyJiStdVPMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118fa3f50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a first try, this is pretty darned good, I think.  Almost 77% of the time, we can correctly predict whether a seriation solution from one of two models belongs to the correct model.  It would be nice to get that accuracy to near perfect if possible, howeve, because the goal here is to examine the fit between an empirical solution and a number of models, and the empirical solution will **never** have arisen from one of our pure theoretical models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Leave-One-Out Cross Validation for Selecting Optimal K ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working on more complex approaches, let's simply make sure we're choosing the optimal number of neighbors for the k-Nearest Neighbors classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-02-23T09:27:56.602081",
     "start_time": "2016-02-23T09:23:35.433791"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test for 1 neighbors: 0.788\n",
      "Accuracy on test for 3 neighbors: 0.798\n",
      "Accuracy on test for 5 neighbors: 0.768\n",
      "Accuracy on test for 7 neighbors: 0.747\n",
      "Accuracy on test for 9 neighbors: 0.758\n",
      "Accuracy on test for 11 neighbors: 0.788\n",
      "Accuracy on test for 15 neighbors: 0.788\n"
     ]
    }
   ],
   "source": [
    "knn = [1, 3, 5, 7, 9, 11, 15]\n",
    "for nn in knn:\n",
    "    gclf = skm.GraphEigenvalueNearestNeighbors(n_neighbors=nn)\n",
    "    test_pred = []\n",
    "    for ix in range(0, len(train_graphs)):\n",
    "        train_loo_graphs, train_loo_labels, test_graph, test_label = leave_one_out_cv(ix, train_graphs, train_labels)\n",
    "        gclf.fit(train_loo_graphs, train_loo_labels)\n",
    "        test_pred.append(gclf.predict([test_graph])[0])\n",
    "    print(\"Accuracy on test for %s neighbors: %0.3f\" % (nn, accuracy_score(train_labels, test_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, 5 was suboptimal, and the best results (at least on these data) are coming from 3-NN classification.  Almost 80%, in fact!  This is great, but let's see what we can do with a more traditional feature engineering approach and a boosted classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "notify_time": "5",
  "toc": {
   "toc_cell": true,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
